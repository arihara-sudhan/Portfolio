[{
		"title": "Jagged Array with Mr.Bean ",
		"logo": "https://media4.giphy.com/media/73E4wQO3OUZPO/200w.gif?cid=82a1493bezs0tirfoorlx9tbhfeu3lfn3x7yrs0o3vrj3seq&rid=200w.gif&ct=g",
		"topic": "Jagged Array with Mr.Bean",
		"num": "0",
		"feeds": ["23-04-2023", "<span>For a single like me, watching Mr.Bean is an essential part of life. In one episode, Bean can’t sleep as usual. There is no Mosquito-Problem but Cat Problem. He gets rid of it by disguising himself as a Dog and barking to deceive the cats. He knows how to solve a problem. (Whether to apply a stack, or a heap , or socks)</span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*42n_Pz3cccW_sWIH5oGC6Q.jpeg\" width=\"95%\"></center>\n<span>Then, he tries to sleep but he can’t. So, he starts counting sheep in a photo. He gets messed with the count. So, to calculate the total number, he uses a calculator and computed row multiplied by the column considering the sheep as an n dimensional array. The problem here is, the column size varies from row to row.</span><br>\n<center><div class=\"iframe\"><iframe width=\"100%\" height=\"100%\" src=\"https://www.youtube.com/embed/FmbmNp1RDCE?controls=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe></div></center>\n<span>He doesn’t care of it. He simply computes the RxC and sleeps. ( Somehow, he slept and the END ) It remembered me the so-called Jagged Array / Ragged Array.So, What is it ? Jagged arrays are the arrays with variable column size. </span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*YXl3u6SjRrzzj0QcjNBMFw.png\" width=\"95%\"></center>\n<span>If we have an array of arrays, we usually work with a fixed size of column. But, there are some situations when we don’t have fixed column size. This is the so called Jagged Array structure. Let’s declare a ragged array in Java. </span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*R7HbbriaoF0DjxUbHOR-dA.png\" width=\"95%\"></center>\n<span>As you can see, we declare the array to have 3 rows and a non-fixed number of columns. Now, let’s say what are the column sizes in different rows. </span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*yER6Prw2DbHI8UzeitlbJA.png\" width=\"95%\"></center>\n<span>So, in first row, there will be 5 elements. And the second row would have 3 and the third would have 2.Now, let’s initialize them using for-loops.</span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*MXSgGARZU6qpErstshrHfQ.png\" width=\"95%\"></center>\n<span>Now, let’s pick the elements of the ragged guy dynamically. It can be used to store rows of data of varying lengths to improve performance when working with multi-dimensional arrays.</span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*RBcREM1LDs_ZzTH6TG3Uig.png\" width=\"95%\"></center>\n<span>USES : It can be used to store rows of data of varying lengths to improve performance when working with multi-dimensional arrays. </span><br>\n"]
	},
	{
		"title": "FAISS Library ",
		"logo": "https://miro.medium.com/v2/resize:fit:640/format:webp/1*5nN-752sNXdmOshueJZaCg.png",
		"topic": "FAISS",
		"num": "1",
		"feeds": ["27-06-2023 ", "<span><center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*5nN-752sNXdmOshueJZaCg.png\" width=\"95%\"></center><span>FAISS is an open-source library developed by Facebook AI Research (FAIR) that provides efficient similarity search and clustering algorithms for high-dimensional vectors. The name “FAISS” stands for “Facebook AI Similarity Search.” It is specifically designed to handle large-scale datasets and is widely used in various applications, including image and text retrieval, recommender systems, and computer vision tasks. <br><h3>Key Features</h3> ⭐ Fast similarity search: FAISS offers highly optimized algorithms for efficient similarity search. It supports both exact and approximate nearest neighbor search, allowing users to trade off between search accuracy and computational efficiency.<br>  ⭐ Indexing structures: FAISS implements several indexing structures to accelerate nearest neighbor search. The most commonly used index structures in FAISS are the Inverted File (IVF) and Product Quantization (PQ) indexes, which are designed to handle high-dimensional vectors efficiently.<br>  ⭐ GPU support: FAISS leverages the computational power of GPUs to accelerate similarity search operations. By utilizing GPUs, FAISS can achieve significant speedups compared to CPU-based implementations, especially for large-scale datasets.<br>  ⭐ Integration with deep learning frameworks: FAISS provides integration with popular deep learning frameworks like PyTorch and TensorFlow. This allows seamless integration of FAISS-based similarity search into existing deep learning pipelines.<br>  ⭐ Extensibility: FAISS is designed to be modular and extensible, allowing users to customize and extend its functionality based on their specific requirements. It provides a flexible API that supports different indexing methods, distance metrics, and search parameters.<br>  ⭐ Community support: FAISS has a vibrant community of users and developers who actively contribute to its development and provide support. It is an open-source project hosted on GitHub, making it accessible to the wider community for contributions and improvements.<br> <h1>Steps for finding Similar Embedding Vector of An Image</h1> Let’s use a use case of finding similar embedding vectors for an image of Ari.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*j9_rRtdpaRax10H8VTFfZw.jpeg\" width=\"95%\"></center><span>Here are the general steps involved:<br>  ⭐ Preparing the dataset : First, you need to have a dataset of images and their corresponding embedding vectors. These embedding vectors are typically obtained by passing the images through a pre-trained deep learning model, such as a convolutional neural network (CNN), which extracts meaningful features from the images.<br>  ⭐ Creating an index : Once you have the dataset and the embedding vectors, you need to create an index using FAISS. The index is a data structure that organizes the vectors in a way that facilitates efficient similarity search. FAISS provides various index structures, but for this example, we’ll use the Inverted File (IVF) index.<br>  ⭐ Adding vectors to the index : You add the embedding vectors of your dataset to the index using the `add` function provided by FAISS. This step builds the index structure based on the given vectors, making subsequent search operations faster.<br>  ⭐ Performing a similarity search : Now that the index is ready, you can perform a similarity search to find the most similar vectors to the image of Ari. You start by obtaining the embedding vector for Ari’s image using the same pre-trained model you used for the dataset.<br>  ⭐ Querying the index : Using the embedding vector of Ari, you pass it to the index’s `search` function in FAISS. This function will efficiently find the nearest neighbors (similar vectors) to the query vector based on a specified distance metric.<br>  ⭐ Analyzing the results : FAISS returns the nearest neighbors along with their distances to the query vector. You can analyze these results to identify the most similar images to Ari based on their embedding vectors. The distances can serve as a measure of similarity, with smaller distances indicating closer similarity.<br>  By following these steps, you can leverage FAISS to efficiently search and find similar embedding vectors for the image of Ari. This process can be applied to a wide range of applications, such as content-based image retrieval, recommendation systems, and more.<br> <h1>An Example</h1>  Our task here is to find out the most similar embedding vector from an index for a given query vector. First, let’s import FAISS using the following command.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*8RQNpBaXx2VPdWMRE4xmZQ.png\" width=\"95%\"></center><span>Secondly, let’s get embedding vectors for each images and store’em all.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*lSzsbvtEU1rDUq4O42GWag.jpeg\" width=\"95%\"></center><span>After we get these embeddings, we have to build an index. I would build an index for the first embeddings collection.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*CyVaNbmvf7ydYxviON0BIw.png\" width=\"95%\"></center><span>After building an index, let’s check out! I would take the first embedding vector of second embeddings collection as the query vector. labels2 has the corresponding labels. As we take the 0th element, the label is labels2[0].  When we search, two array values are returned. First array (D) holds the distances and the second array (I) holds the positions of the embedding vectors which are found to be so close. The first value in each are considered to be very close to the query. Let’s check whether the labels1[I[0][0]] equals to labels2[0] or not.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*_h4LMBtJwln8fx8sLiMaHg.png\" width=\"95%\"></center><span>True they are…<br> <h1>Index Types</h1>  FAISS provides various types of indexes that can be used for different similarity search scenarios. Here are a few commonly used index types in FAISS :<br>  ⭐ IndexFlatL2 : This index type implements a brute-force approach for similarity search using the L2 (Euclidean) distance metric. It compares the query vector with all vectors in the index to find the nearest neighbors. IndexFlatL2 is simple and easy to use, but it can be computationally expensive for large datasets. <br> ⭐ IndexIVFFlat : The Inverted File (IVF) index is designed for approximate nearest neighbor search. It divides the vector space into multiple cells and assigns vectors to these cells. IndexIVFFlat uses an inverted file structure to store these assignments efficiently. During search, only a subset of cells is examined, which significantly speeds up the search process. <br> ⭐ IndexHNSWFlat : The Hierarchical Navigable Small World (HNSW) index is a graph-based index that organizes vectors in a hierarchical manner. It builds a graph where each vector is connected to other vectors based on their proximity. This index structure allows for efficient and scalable approximate nearest neighbor search. <br> ⭐ IndexLSH : The Locality-Sensitive Hashing (LSH) index is suitable for very high-dimensional vectors. It uses hashing techniques to map similar vectors to the same or nearby buckets. During search, only the relevant buckets are examined, reducing the search space. IndexLSH provides approximate nearest neighbor search with trade-offs between accuracy and speed. <br> These are just a few examples of index types available in Faiss. Depending on your specific use case, we can choose the appropriate index type and configure its parameters to achieve the desired balance between search accuracy and computational efficiency.<br> <h1>Implementation of All</h1></span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*M5R6pFOgM3c4I-2bbSq42w.png\" width=\"95%\"></center><span>Let’s check which one performs well for this usecase.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*9LOI0FJSXXOUi5jDQO-_Dg.png\" width=\"95%\"></center><span><h1>Under The Hood</h1>      1.Multi-threading to exploit multiple cores and perform parallel searches on multiple GPUs.     <br>2.BLAS libraries for efficient exact distance computations via matrix/matrix multiplication. An efficient brute-force implementation cannot be optimal without using BLAS. BLAS/LAPACK is the only mandatory software dependency of FAISS.<br>     3.Machine SIMD vectorization and popcount are used to speed up distance computations for isolated vectors. <br> <h1>MERCI</h1></span><br></span>"]
	},

	{
		"title": "PreTraining & FineTuning ",
		"logo": "https://media.istockphoto.com/id/1154814568/vector/vector-outline-gloriosa-superba-or-flame-lily-or-glory-lily-tropical-flower-head-in-red-and.jpg?s=612x612&w=0&k=20&c=jfyQm1qCRkc5HGm_8_kO0otcS-GRytK09ESrIFOqLWk=",
		"topic": "PreTraining & FineTuning ",
		"num": "2",
		"feeds": ["21-07-2023", "<span>\nIn this article, I would explain the concepts of Pre-Training and Fine-Tuning with Glory Lily.</span><br>\n<img src=\"https://s3.amazonaws.com/eit-planttoolbox-prod/media/images/GloriRo.jpg\">\n<h3>PreTraining</h3><hr>\n<span>Let's imagine that we want to teach an AI Model to understand Glory Lily The Flower. Initially, this model knows nothing about Glory Lily. So, from the scratch, we've to embark! We need to show the AI Model a large corpus or a massive collection of Glory Lily related texts such as texts in Books, Articles and so on. The AI Model reads through this vast collection and learns the various aspects of Glory Lily such as the appearance, aroma, chroma, taste and so on. To make this learning more engaging, we are going for MASKING of some words in our corpus so that the AI Model can find the missing word based on the context. And note that, we don't do masking at the very beginning of our training. Because, if we do that at the bery beginning, the AI Model will perform poorly as it would be yet to catch some(more) patterns from the large corpus.🥺</span><br>\n<b>This is how we play that MASKING Game</b><br>\n<span><b>Original Sentence:</b> \"Glory Lily is also called as Flame Lily.\"<br>\n<b>Masked Sentence:</b> \"Glory Lily is also called as [MASK].\"</span><br>\n<img src=\"https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExcTFqbHo5aGZocnFweWE2NHdsdXk1NHpyNnIzeWM4eHR0ZXF1Z2I1ZSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9cw/WCzGme5RtmUM7Fhl9f/giphy.gif\"><br>\n<span>The AI would guess the missing word \"Flame Lily\" based on the context of the sentence. Through this process, the AI learns to understand various Glory Lily-related patterns and relationships. It grasps that Glory Lilies are related to flowers, stems, tubes and some other features. The AI Model also starts to learn about sentence structure and how words work together to convey meaning.</span>\n<br>\n<h3>FineTuning</h3><hr>\n<span>\nNow, after this pre-training phase, the AI has gained a general understanding of Glory Lily. However, it hasn't yet learned to perform specific tasks related to Glory Lily, like answering questions about Glory Lily Medicines or identifying different types of Glory Lilies. This is where fine-tuning comes in.</span>\n<span>\nIn the fine-tuning phase, we provide the AI Model with more specific tasks related to Glory Lilies. For instance, you might give it a dataset with questions and answers about Glory Lilies. The AI Model then adjusts its understanding of Glory Lilies based on this more focused data. This fine-tuning process helps the AI Model specialize in Glory Lily-related tasks and improves its performance on those tasks.🤓</span>\n<span>The process of pre-training is like giving the AI a broad understanding of Glory Lilies by exposing it to a large collection of Glory Lily-related texts and playing a context-based word-guessing game. Fine-tuning, on the other hand, is like providing more targeted training data to help the AI Model excel in specific Glory Lily-related tasks. Together, these two steps form a powerful approach to train AI Models and enable them to perform well on various language understanding tasks.</span><br>\n<img src=\"https://media.istockphoto.com/id/1251181057/photo/gloriosa-superba-or-climbing-lily.jpg?s=612x612&w=0&k=20&c=SDd9awsxm4gltaaMNC9NPhXTU3vwO5KGnk1sElzHiWk=\"><br><br>\n<b>What is the state flower of TamilNadu?</b><br>\n<b>AI Model: Glory Lily</b>\n<center>\n  <div class=\"container\">\n    <div class=\"upper-part\">\n      <div class=\"circle red\"></div>\n      <div class=\"circle green\"></div>\n      <div class=\"circle blue\"></div>\n    </div>\n\n    <div class=\"code\">\n    function greet() {\n      //I am a code here\n      console.log(\"Hello, world!\");\n    }\n    </div>\n  </div>\n</center>\n"]
	},


	{
		"title": "Neural Network ",
		"logo": "https://media3.giphy.com/media/xT9IgN8YKRhByRBzMI/giphy.gif?cid=6c09b952fssombz2mzcueme48zefw99n75u6ifr1wewupjvl&ep=v1_internal_gif_by_id&rid=giphy.gif&ct=g",
		"topic": "Neural Network ",
		"num": "3",
		"feeds": ["28-07-2023", "<span>Neural Network is indeed one of the most remarkable discoveries in human history. It operates in a fascinating and almost magical manner. Just like a baby learns from its experiences, a neural network learns from data. But what does learning mean in the context of a neural network? Let's explore this concept in simple terms.</span>\n<span>In the Southern Boy's 3D Animation, he illustrates how a neural network works, making the learning process more accessible. The golden spheres represent neurons, which are the building blocks of a neural network.</span><br>\n<div class=\"iframe\"><iframe width=\"100%\" height=\"100%\" src=\"https://www.youtube.com/embed/IwTxj6X36w8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe></div><br>\n<span>Learning in a neural network occurs through a process called \"training.\" During training, the network is exposed to a large amount of input data along with the corresponding correct output or label. The network's task is to learn patterns and relationships within the data so that it can accurately map inputs to the correct outputs.</span><br>\n<h3>USE-CASE : XOR</h3><hr>\n<span>Brace yourself for an enchanting adventure into the realm of neural networks, where intelligence and intuition converge in the pursuit of understanding this fascinating phenomenon. We all know what is XOR Operation. It gives 1 when inputs are different. It gives 0 when inputs are same.</span><br>\n<h3>LAYERS</h3><hr>\n<span>A Neural Network usually has three layers of Input Layer, Hidden Layer and Output Layer. There can be more hidden layers. They can be increased to a large extent for a subtle performance but it can also lead to some potential risks.</span><br>\n<h3>WEIGHTS AND BIASES : RANDOMNESS</h3><hr>\n<span>Imagine we are trying to tune our FM radio to find a specific channel (\"FM Rainbow\"). However, we have no idea where the exact frequency of this channel is, so we start with the radio knobs set to random positions. In the world of neural networks, similar to our radio tuning, we begin with random weights and biases. These weights and biases are like the radio knobs, but instead of controlling the radio frequency, they control the behavior of the neural network. So, initially, these weights and biases are randomly initialized.</span><br>\n<h3>FORWARD PROPAGATION</h3><hr>\n<span>Now, we turn on the radio and start scanning through different frequencies. As we twist the knobs randomly, we might not immediately land on the FM Rainbow channel. But every time we try a new random setting, we pay attention to the sound we hear. Gradually, we start adjusting the knobs based on the sound, moving closer and closer to the FM Rainbow channel. In a neural network, we do something similar during the learning process. The network starts with random weights and biases, and it takes in some input data.</span><br>\n<span>Each input is multiplied by its corresponding weight. These weights represent the strengths of connections between the inputs and the neuron. The multiplication of inputs and weights determines how much influence each input has on the neuron's output. The results of the multiplications (inputs multiplied by weights) are then added together to form a single value. In essence, the inputs are individually adjusted by their corresponding weights and then combined to create a single value, which serves as the input for the activation function.\nThe activation function in a neural network is like a switch that decides whether a neuron should be \"activated\" or \"turned on\" based on the input it receives. Just like how a light switch can either turn on the light or keep it off, the activation function determines if a neuron should fire or remain silent. When the input to a neuron passes through the activation function, it transforms the input in a certain way. If the transformed value is above a certain threshold, the neuron becomes active and sends its output to the next layer of the neural network. On the other hand, if the transformed value is below the threshold, the neuron stays inactive, and its output is not propagated further. Here, in our illustration, we use sigmoid activation function. There is a plethora of activation functions. This non-linear behavior introduced by the activation function allows neural networks to learn and capture complex patterns in the data. It's like introducing flexibility in decision-making, as neurons can either contribute significantly to the final output or not at all, depending on the transformed value.</span><br>\n<h3>LOSS</h3><hr>\n<span>Just like we kept tweaking the radio knobs and listened to the sound, the neural network does something similar with its predictions. It compares its predictions with the actual correct answers (labels) for the input data and measures how far off it was. This difference is called the \"error\" or \"loss.\" For XOR Operation, when inputs are 0 and 1, the output must be 1. But, what we get here is not 1. So, we measure the distance between the actual value and the predicted value to take a appropriate action. Here, Mean Squared Error is used. There is also a plethora of loss functions.</span><br>\n<h3>THE MAGIC</h3><hr>\n<span>The magic happens when the neural network uses a clever algorithm called \"backpropagation\" to adjust those random weights and biases based on the error it calculated. By doing this over and over again with different data, the network learns from its mistakes and gradually gets better at making predictions, just like we got closer to the FM Rainbow channel by fine-tuning the knobs.</span> <br>\n<pre style=\"color: #16FF00;\">\n1. Input Layer Neurons (x1=1, x2=0)\n   - Neuron x1: The input value is 1.\n   - Neuron x2: The input value is 0.\n\n2. Hidden Layer Neuron (h):\n   - Input: \n     - Neuron x1 sends its output (1)\n       through weight w1 (0.3).\n     - Neuron x2 sends its output (0)\n       through weight w2 (0.5).\n     - Adding bias b1 (0.1).\n     - Calculate the weighted sum:\n       z1 = (1 * 0.3) + (0 * 0.5) + 0.1\n          = 0.4\n   - Activation:\n     - The sigmoid activation function:\n       h = 1 / (1 + exp(-z1))\n       ≈ 0.59874\n\n3. Output Layer Neuron (o):\n   - Input: \n     - Neuron h sends its output (0.59874)\n       through weight w3 (0.2).\n     - Neuron h sends its output (0.59874)\n       through weight w4 (0.4).\n     - Adding bias b2 (0.6).\n     - Calculate the weighted sum:\n       z2 = (0.59874*0.2)+(0.59874*0.4)+0.6\n        ≈ 0.959244\n   - Activation:\n     - The sigmoid activation function:\n       o = 1 / (1 + exp(-z2)) ≈ 0.7222\n\n4. Loss Calculation:\n   - Given that the target output for XOR with\n     inputs (1, 0) is 1, the actual output is\n     approximately 0.7222.\n   - Calculate the Mean Squared Error loss:\n     L = (1/2) * (o - y_true)^2\n       ≈ (1/2) * (0.7222 - 1)^2\n       ≈ 0.03858\n\n5. Backpropagation:\n   - Derivative of the loss with respect to\n     the output layer input (z2):\n     ∂L/∂z2 = (o - y_true) * o * (1 - o)\n     ≈ (0.7222 - 1) * 0.7222 * (1 - 0.7222)\n     ≈ -0.14719\n\n   - Derivative of the loss with respect to\n     the hidden layer output (h):\n     ∂L/∂h = ∂L/∂z2 * w3\n     ≈ -0.14719 * 0.2 ≈ -0.02944\n\n   - Derivative of the loss with respect to\n     the input layer weights and bias:\n     ∂L/∂w1 = ∂L/∂h * h * (1 - h) * x1\n     ≈ -0.02944*0.59874*(1 - 0.59874)*1\n     ≈ -0.00783\n     ∂L/∂w2 = ∂L/∂h * h * (1 - h) * x2\n     ≈ -0.02944 * 0.59874*(1 - 0.59874)*0\n     ≈ 0\n     ∂L/∂b1 = ∂L/∂h * h * (1 - h)\n     ≈ -0.02944*0.59874 * (1 - 0.59874)\n     ≈ -0.01108\n\n   - Derivative of the loss w.r.to the o/p\n     layer weights and bias:\n     ∂L/∂w3 = ∂L/∂z2 * h\n     ≈ -0.14719 * 0.59874\n     ≈ -0.08812\n     ∂L/∂w4 = ∂L/∂z2 * h\n     ≈-0.14719 * 0.59874\n     ≈ -0.08812\n     ∂L/∂b2 = ∂L/∂z2\n     ≈ -0.14719\n\n6. Update Weights and Biases:\n   - Learning rate α = 0.1.\n   - New weights and biases:\n     w1_new = w1 - α * ∂L/∂w1\n     ≈ 0.3 - 0.1 * (-0.00783)\n     ≈ 0.300783\n     w2_new = w2 - α * ∂L/∂w2\n     ≈ 0.5 - 0.1 * 0 ≈ 0.5\n     w3_new = w3 - α * ∂L/∂w3\n     ≈ 0.2 - 0.1 * (-0.08812) \n     ≈ 0.208812\n     w4_new = w4 - α * ∂L/∂w4 \n     ≈ 0.4 - 0.1 * (-0.08812) \n     ≈ 0.408812\n     b1_new = b1 - α * ∂L/∂b1 \n     ≈ 0.1 - 0.1 * (-0.01108) \n     ≈ 0.100111\n     b2_new = b2 - α * ∂L/∂b2 \n     ≈ 0.6 - 0.1 * (-0.14719) \n     ≈ 0.614719\n </pre>\n<br>\n<span>These updated values of weights and biases are used for forward propagation.</span><br>\n<span>Eventually, after many rounds of adjusting and fine-tuning, the neural network's weights and biases settle into values that help it make highly accurate predictions for the FM Rainbow channel (or any other task it was designed for). This process of finding the best weights and biases for the neural network is what we call \"training,\" and it allows the neural network to uncover hidden patterns and relationships in the data, much like how you discovered the FM Rainbow channel by tuning your radio.</span><br>"]
	},


	{
		"title": "CNN ",
		"logo": "https://t4.ftcdn.net/jpg/02/78/02/03/360_F_278020343_c78GsSY9qRpwGLIUYl2VZZXjBQuvm9FF.jpg",
		"topic": "CNN ",
		"num": "4",
		"feeds": ["10-08-2023", "<span>Once upon a time in the land of Deep Learning Forest, there lived a wise and ancient tortoise named Conv. Conv was known far and wide for his slow yet steady approach to solving complex problems. He had an insatiable curiosity for learning patterns and recognizing shapes in the environment around him.</span><br>\n<img src=\"https://cdn.pixabay.com/photo/2021/07/07/05/01/tortoise-6393243_960_720.png\"> <br>\n<h3>Dataset</h3><hr>\n<span>One day, as he was wandering through the forest, Conv came across a beautiful collection of images scattered on the forest floor. These images had various objects, creatures, and landscapes, and they seemed to be filled with fascinating patterns that intrigued Conv's inquisitive mind. Curious to learn more about the images, Conv decided to embark on a journey to uncover the hidden secrets within them. He knew that his wisdom and slow pace would be a perfect match for this daunting task.</span><br>\n<img src=\"https://yingqianwang.github.io/Flickr1024/pics/Flickr1024.jpg\">\n<h3>Kernels</h3><hr>\n<span>\nAs he took his first step, Conv encountered a group of young creatures known as Filters. These Filters were specialized in detecting specific patterns in the images, such as edges, colors, and textures. They could slide across the images, peeking at small portions at a time, and signaling their findings to Conv. With the help of the Filters, Conv learned about the low-level features of the images, the basic building blocks that formed the objects and shapes. These low-level features were fascinating, but Conv knew he needed to go deeper into the forest of knowledge.</span><br>\n<img src=\"https://mlnotebook.github.io/img/CNN/convSobel.gif\">\n<h3>Convolutional Layer</h3><hr>\n<span>As Conv delved further, he met the powerful Convolution Layer, a magnificent structure with a vast array of Filters. The Convolution Layer could analyze the entire image, using multiple Filters to capture different patterns simultaneously. The Filters' collective wisdom helped Conv recognize more complex shapes and objects within the images. But the forest held even more secrets.</span><br>\n<h3>Activation Function</h3><hr>\n<span>Conv stumbled upon a mysterious creature called Activation Function, a mystical gatekeeper that had the power to determine which features should be emphasized and which should be discarded. Activation Function guided Conv in his exploration, making sure he stayed on the right path.</span><br>\n<h3>Pooling Layer</h3><hr>\n<span>\nIn the heart of the forest, Conv encountered the Pooling Ponds, tranquil places where he could rest and reflect. The Pooling Ponds had a unique ability - they could reduce the size of the feature maps, making the images more manageable. This allowed Conv to focus on the most important aspects while discarding the less crucial details. As he moved forward, Conv felt he was getting closer to the ultimate truth hidden within the images.</span><br>\n<img src=\"https://mlnotebook.github.io/img/CNN/poolfig.gif\">\n<h3>Fully Connected Layer</h3><hr>\n<span>He arrived at the Fully Connected Forest, a place where the features he had learned so far were combined and analyzed, leading him to make decisions and predictions about the images' contents. At the end of his long and exciting journey, Conv realized that he had achieved something remarkable. By combining the knowledge gained from Filters, Convolution Layers, Activation Functions, and Pooling Ponds, he had constructed a remarkable structure - a Convolutional Neural Network or CNN.</span><br>\n<h3>CNN</h3><hr>\n<span>With his newfound wisdom, Conv could now analyze images, recognize objects, and even predict the class of the creatures or objects depicted in the images. He knew that his slow and methodical approach had led him to a deeper understanding of the world around him, and he was ready to share his knowledge with others.</span><br>\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*O3_M1yjincYZ4b2AIr9xzw.jpeg\">\n<br><span>From that day on, Conv continued his journey through the vast Deep Learning Forest, helping others and inspiring many to embrace the power of CNNs—the slow but mighty tortoises of the machine learning world. And so, the tale of Conv, the wise tortoise of CNN, spread far and wide, inspiring generations of learners to seek wisdom in the patterns of the world.</span><br>\n<center><h1>MERCI</h1></center>"]
	},


	{
		"title": "Self Attention ",
		"logo": "https://globus.am/uploads/FidgetSpinner.gif",
		"topic": "Self Attention ",
		"num": "5",
		"feeds": ["21-08-2023", "<span>Context Matters! Yes... Context should be considered in Text Processing. Why? When we have a sentence, \"I see fishes by the bank\", The Question is, does the word bank here represent the financial building or the edge of a waterbody like river? Answering this question is quite easy for a human like Ari.😎 We simply look on the words near to the word, bank. We see \"fishes\"! Ahh! Since fishes swims only in a waterbody like river, the word bank here in this context definitely refers to the edge of a waterbody like river! We made it!</span>\n<img src=\"https://media.tenor.com/Svk4rFVzYSsAAAAd/fish-what.gif\">\n<span>Now, we could realize why context matters! In language modelling tasks, it plays a major role to consider the context. Self Attention adds contextual information to the lower level representations fed to the model being trained. Besides, Self Attention captures long range dependencies by capturing the contextually important details by weighing. Alright! Let's learn the simple steps followed in the process of Self Attention.</span>\n<div class=\"iframe\">\n<iframe width=\"100%\" height=\"100%\" src=\"https://www.youtube.com/embed/bx-RgoHEadE?controls=0\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n</div>\n<h4>TOKENIZATION</h4><hr>\n<span>Tokenization is a fundamental-first step that involves breaking down a piece of text into smaller units called tokens. Tokens can be words, subwords, or even characters, depending on the level of granularity needed for a specific task.</span>\n<h4>VECTORIZATION</h4><hr>\n<span>Vectorization techniques like Word2Vec are essential. It is a simple process in which Tokens are converted into Vectors. Machines, including neural networks, process numbers more effectively than raw text. Word2Vec and similar techniques convert words into dense numerical vectors, making it possible to use them as input for machine learning models. Word2Vec captures semantic relationships between words. Words with similar meanings have vectors that are closer together in the vector space. This allows NLP models to understand and leverage word semantics, which is crucial for various NLP tasks.</span>\n<h4>DOT PRODUCT SCORES</h4><hr>\n<span>In self-attention, we perform a dot product between pairs of words' vectors to measure how related or similar they are to each other. This dot product helps the model decide how much attention to give to each word when processing a specific word in a sequence. It allows the model to capture relationships and dependencies between words, which is essential for understanding context and improving the performance.</span>\n<img src=\"https://tensorflow.google.cn/images/spanish-english.png?hl=zh-cn\" style=\"background-color: white\">\n<pre style=\"color: #16FF00;\">\n\"I\" with \"I\": 0.2×0.2+0.4×0.4+0.1×0.1\n=0.090.2×0.2+0.4×0.4+0.1×0.1=0.09\n\"I\" with \"fishes\": 0.2×0.5+0.4×0.7+0.1×0.2\n =0.390.2×0.5+0.4×0.7+0.1×0.2=0.39\n\"I\" with \"bank\": 0.2×0.1+0.4×0.3+0.1×0.6\n =0.180.2×0.1+0.4×0.3+0.1×0.6=0.18\nSimilarly, calculate dot products\nfor other words.\n</pre>\n<h4>NORMALIZATION</h4><hr>\n<span>Normalization is performed after the dot product in self-attention mechanisms, specifically through the use of the softmax function. The dot product between two vectors can result in values that vary in magnitude. In deep neural networks, especially with many layers, this can lead to issues like vanishing gradients or exploding gradients during training. Normalization helps stabilize these values by ensuring they are within a reasonable range. The softmax function transforms the raw dot products into a probability distribution. This is important for interpretability because it allows us to see how much attention each word should receive relative to others. By using a probability distribution, we can ensure that the attention scores are meaningful and comparable.</span>\nApply softmax to the scaled dot products to get attention scores, ensuring they sum to 1:\n<pre style=\"color: #16FF00;\">\nFor \"I\":\nsoftmax([0.09/1.732,0.39/1.732,0.18/1.732])\n≈[0.247,0.545,0.208]\nsoftmax([0.09/1.732,0.39/1.732,0.18/1.732])\n≈[0.247,0.545,0.208]\nSimilarly, calculate softmax for other words.\n</pre>\n<h4>WEIGH THE ORIGINAL VECTORS</h4><hr>\n<span>This step allows the model to create a new representation for each word in the sequence, incorporating information from other words based on their attention weights. For each word in the sequence, we have the normalized attention scores obtained from the softmax operation, representing how much attention each word should receive from the current word. We multiply each value vector (V) by its corresponding attention score (after softmax) to calculate a weighted sum. This weighted sum captures the attended information from other words for the current word.\nMathematically, for each word position i, the weighted sum is calculated as follows:\n<pre style=\"color: #16FF00;\">\nWeighted Sumi​=\n∑j​(Attention Scorei,j​×Value Vectorj​)\nCalculate the weighted sum for each word by\nmultiplying the attention scores with\nthe value vectors:\n\nFor \"I\": [0.247×0.2,0.545×0.4,0.208×0.1]\n=[0.049,0.218,0.021]\nSimilarly, calculate weighted sums for\nother words.\n</pre>\n<span>In self-attention, for each word in the input sequence, we create three types of vectors: Query (Q), Key (K), and Value (V) vectors. These vectors are used to calculate the attention scores between words.<br>\n<center>SENTENCE : \"I see fishes by the bank\"</center>\n<br>\nNow, let's generate Query (Q), Key (K), and Value (V) vectors for each word:\n<span>\nQuery (Q) Vectors:<br>\nQuery vectors represent the current word and are used to determine how much attention to give to other words.\nFor each word, the query vector is typically generated by applying a linear transformation (a learned weight matrix) to the word's initial representation.\n<br>\nFor example:<br>\n\"I\" (Query Q_I): A linear transformation produces a query vector.\n\"see\" (Query Q_see): Another linear transformation produces a query vector.\nSimilarly, query vectors are generated for \"fishes,\" \"by,\" and \"bank.\"\n<br>\nKey (K) Vectors:<br>\nKey vectors represent other words in the sequence and are used to calculate the compatibility or similarity between the current word and other words.\nLike query vectors, key vectors are generated for each word by applying a linear transformation to the word's initial representation.\n<br>\nFor example:<br>\n\"I\" (Key K_I): A linear transformation produces a key vector.\n\"see\" (Key K_see): Another linear transformation produces a key vector.\nSimilarly, key vectors are generated for \"fishes,\" \"by,\" and \"bank.\"\n<br>\nValue (V) Vectors:<br>\nValue vectors represent the information in other words that should be attended to by the current word.\nAs with query and key vectors, value vectors are generated by applying a linear transformation to the word's initial representation.\n<br>\nFor example:<br>\n\"I\" (Value V_I): A linear transformation produces a value vector.\n\"see\" (Value V_see): Another linear transformation produces a value vector.\nSimilarly, value vectors are generated for \"fishes,\" \"by,\" and \"bank.\"\n</span>\n<span>For each word in the sentence, we have Query (Q), Key (K), and Value (V) vectors. These vectors are used in the self-attention mechanism to calculate the attention scores, which determine how much each word should attend to the others in the context of the sentence. This enables the model to understand the relationships and dependencies between words in the sequence, leading to context-aware representations.</span>\n<br>\n<center><h1>NANDRI</h1></center>"]
	}
]
