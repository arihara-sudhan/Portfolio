[
  {
    "title": "Jagged Array with Mr.Bean ",
    "logo": "https://media4.giphy.com/media/73E4wQO3OUZPO/200w.gif?cid=82a1493bezs0tirfoorlx9tbhfeu3lfn3x7yrs0o3vrj3seq&rid=200w.gif&ct=g",
    "topic": "Jagged Array with Mr.Bean",
    "num": "0",
    "feeds": [
      "23-04-2023",
      "<span>For a single like me, watching Mr.Bean is an essential part of life. In one episode, Bean can’t sleep as usual. There is no Mosquito-Problem but Cat Problem. He gets rid of it by disguising himself as a Dog and barking to deceive the cats. He knows how to solve a problem. (Whether to apply a stack, or a heap , or socks)</span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*42n_Pz3cccW_sWIH5oGC6Q.jpeg\" width=\"95%\"></center>\n<span>Then, he tries to sleep but he can’t. So, he starts counting sheep in a photo. He gets messed with the count. So, to calculate the total number, he uses a calculator and computed row multiplied by the column considering the sheep as an n dimensional array. The problem here is, the column size varies from row to row.</span><br>\n<center><div class=\"iframe\"><iframe width=\"100%\" height=\"100%\" src=\"https://www.youtube.com/embed/FmbmNp1RDCE?controls=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe></div></center>\n<span>He doesn’t care of it. He simply computes the RxC and sleeps. ( Somehow, he slept and the END ) It remembered me the so-called Jagged Array / Ragged Array.So, What is it ? Jagged arrays are the arrays with variable column size. </span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*YXl3u6SjRrzzj0QcjNBMFw.png\" width=\"95%\"></center>\n<span>If we have an array of arrays, we usually work with a fixed size of column. But, there are some situations when we don’t have fixed column size. This is the so called Jagged Array structure. Let’s declare a ragged array in Java. </span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*R7HbbriaoF0DjxUbHOR-dA.png\" width=\"95%\"></center>\n<span>As you can see, we declare the array to have 3 rows and a non-fixed number of columns. Now, let’s say what are the column sizes in different rows. </span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*yER6Prw2DbHI8UzeitlbJA.png\" width=\"95%\"></center>\n<span>So, in first row, there will be 5 elements. And the second row would have 3 and the third would have 2.Now, let’s initialize them using for-loops.</span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*MXSgGARZU6qpErstshrHfQ.png\" width=\"95%\"></center>\n<span>Now, let’s pick the elements of the ragged guy dynamically. It can be used to store rows of data of varying lengths to improve performance when working with multi-dimensional arrays.</span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*RBcREM1LDs_ZzTH6TG3Uig.png\" width=\"95%\"></center>\n<span>USES : It can be used to store rows of data of varying lengths to improve performance when working with multi-dimensional arrays. </span><br>\n"
    ]
  },
  {
    "title": "FAISS Library ",
    "logo": "https://miro.medium.com/v2/resize:fit:640/format:webp/1*5nN-752sNXdmOshueJZaCg.png",
    "topic": "FAISS",
    "num": "2",
    "feeds": [
      "27-06-2023 ",
      "<span><center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*5nN-752sNXdmOshueJZaCg.png\" width=\"95%\"></center><span>FAISS is an open-source library developed by Facebook AI Research (FAIR) that provides efficient similarity search and clustering algorithms for high-dimensional vectors. The name “FAISS” stands for “Facebook AI Similarity Search.” It is specifically designed to handle large-scale datasets and is widely used in various applications, including image and text retrieval, recommender systems, and computer vision tasks. <br><h3>Key Features</h3> ⭐ Fast similarity search: FAISS offers highly optimized algorithms for efficient similarity search. It supports both exact and approximate nearest neighbor search, allowing users to trade off between search accuracy and computational efficiency.<br>  ⭐ Indexing structures: FAISS implements several indexing structures to accelerate nearest neighbor search. The most commonly used index structures in FAISS are the Inverted File (IVF) and Product Quantization (PQ) indexes, which are designed to handle high-dimensional vectors efficiently.<br>  ⭐ GPU support: FAISS leverages the computational power of GPUs to accelerate similarity search operations. By utilizing GPUs, FAISS can achieve significant speedups compared to CPU-based implementations, especially for large-scale datasets.<br>  ⭐ Integration with deep learning frameworks: FAISS provides integration with popular deep learning frameworks like PyTorch and TensorFlow. This allows seamless integration of FAISS-based similarity search into existing deep learning pipelines.<br>  ⭐ Extensibility: FAISS is designed to be modular and extensible, allowing users to customize and extend its functionality based on their specific requirements. It provides a flexible API that supports different indexing methods, distance metrics, and search parameters.<br>  ⭐ Community support: FAISS has a vibrant community of users and developers who actively contribute to its development and provide support. It is an open-source project hosted on GitHub, making it accessible to the wider community for contributions and improvements.<br> <h1>Steps for finding Similar Embedding Vector of An Image</h1> Let’s use a use case of finding similar embedding vectors for an image of Ari.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*j9_rRtdpaRax10H8VTFfZw.jpeg\" width=\"95%\"></center><span>Here are the general steps involved:<br>  ⭐ Preparing the dataset : First, you need to have a dataset of images and their corresponding embedding vectors. These embedding vectors are typically obtained by passing the images through a pre-trained deep learning model, such as a convolutional neural network (CNN), which extracts meaningful features from the images.<br>  ⭐ Creating an index : Once you have the dataset and the embedding vectors, you need to create an index using FAISS. The index is a data structure that organizes the vectors in a way that facilitates efficient similarity search. FAISS provides various index structures, but for this example, we’ll use the Inverted File (IVF) index.<br>  ⭐ Adding vectors to the index : You add the embedding vectors of your dataset to the index using the `add` function provided by FAISS. This step builds the index structure based on the given vectors, making subsequent search operations faster.<br>  ⭐ Performing a similarity search : Now that the index is ready, you can perform a similarity search to find the most similar vectors to the image of Ari. You start by obtaining the embedding vector for Ari’s image using the same pre-trained model you used for the dataset.<br>  ⭐ Querying the index : Using the embedding vector of Ari, you pass it to the index’s `search` function in FAISS. This function will efficiently find the nearest neighbors (similar vectors) to the query vector based on a specified distance metric.<br>  ⭐ Analyzing the results : FAISS returns the nearest neighbors along with their distances to the query vector. You can analyze these results to identify the most similar images to Ari based on their embedding vectors. The distances can serve as a measure of similarity, with smaller distances indicating closer similarity.<br>  By following these steps, you can leverage FAISS to efficiently search and find similar embedding vectors for the image of Ari. This process can be applied to a wide range of applications, such as content-based image retrieval, recommendation systems, and more.<br> <h1>An Example</h1>  Our task here is to find out the most similar embedding vector from an index for a given query vector. First, let’s import FAISS using the following command.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*8RQNpBaXx2VPdWMRE4xmZQ.png\" width=\"95%\"></center><span>Secondly, let’s get embedding vectors for each images and store’em all.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*lSzsbvtEU1rDUq4O42GWag.jpeg\" width=\"95%\"></center><span>After we get these embeddings, we have to build an index. I would build an index for the first embeddings collection.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*CyVaNbmvf7ydYxviON0BIw.png\" width=\"95%\"></center><span>After building an index, let’s check out! I would take the first embedding vector of second embeddings collection as the query vector. labels2 has the corresponding labels. As we take the 0th element, the label is labels2[0].  When we search, two array values are returned. First array (D) holds the distances and the second array (I) holds the positions of the embedding vectors which are found to be so close. The first value in each are considered to be very close to the query. Let’s check whether the labels1[I[0][0]] equals to labels2[0] or not.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*_h4LMBtJwln8fx8sLiMaHg.png\" width=\"95%\"></center><span>True they are…<br> <h1>Index Types</h1>  FAISS provides various types of indexes that can be used for different similarity search scenarios. Here are a few commonly used index types in FAISS :<br>  ⭐ IndexFlatL2 : This index type implements a brute-force approach for similarity search using the L2 (Euclidean) distance metric. It compares the query vector with all vectors in the index to find the nearest neighbors. IndexFlatL2 is simple and easy to use, but it can be computationally expensive for large datasets. <br> ⭐ IndexIVFFlat : The Inverted File (IVF) index is designed for approximate nearest neighbor search. It divides the vector space into multiple cells and assigns vectors to these cells. IndexIVFFlat uses an inverted file structure to store these assignments efficiently. During search, only a subset of cells is examined, which significantly speeds up the search process. <br> ⭐ IndexHNSWFlat : The Hierarchical Navigable Small World (HNSW) index is a graph-based index that organizes vectors in a hierarchical manner. It builds a graph where each vector is connected to other vectors based on their proximity. This index structure allows for efficient and scalable approximate nearest neighbor search. <br> ⭐ IndexLSH : The Locality-Sensitive Hashing (LSH) index is suitable for very high-dimensional vectors. It uses hashing techniques to map similar vectors to the same or nearby buckets. During search, only the relevant buckets are examined, reducing the search space. IndexLSH provides approximate nearest neighbor search with trade-offs between accuracy and speed. <br> These are just a few examples of index types available in Faiss. Depending on your specific use case, we can choose the appropriate index type and configure its parameters to achieve the desired balance between search accuracy and computational efficiency.<br> <h1>Implementation of All</h1></span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*M5R6pFOgM3c4I-2bbSq42w.png\" width=\"95%\"></center><span>Let’s check which one performs well for this usecase.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*9LOI0FJSXXOUi5jDQO-_Dg.png\" width=\"95%\"></center><span><h1>Under The Hood</h1>      1.Multi-threading to exploit multiple cores and perform parallel searches on multiple GPUs.     <br>2.BLAS libraries for efficient exact distance computations via matrix/matrix multiplication. An efficient brute-force implementation cannot be optimal without using BLAS. BLAS/LAPACK is the only mandatory software dependency of FAISS.<br>     3.Machine SIMD vectorization and popcount are used to speed up distance computations for isolated vectors. <br> <h1>MERCI</h1></span><br></span>"
    ]
  },
  {
    "title": "FAISS Library ",
    "logo": "https://miro.medium.com/v2/resize:fit:640/format:webp/1*5nN-752sNXdmOshueJZaCg.png",
    "topic": "FAISS",
    "num": "3",
    "feeds": [
      "27-06-2023 ",
      "<span><center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*5nN-752sNXdmOshueJZaCg.png\" width=\"95%\"></center><span>FAISS is an open-source library developed by Facebook AI Research (FAIR) that provides efficient similarity search and clustering algorithms for high-dimensional vectors. The name “FAISS” stands for “Facebook AI Similarity Search.” It is specifically designed to handle large-scale datasets and is widely used in various applications, including image and text retrieval, recommender systems, and computer vision tasks. <br><h3>Key Features</h3> ⭐ Fast similarity search: FAISS offers highly optimized algorithms for efficient similarity search. It supports both exact and approximate nearest neighbor search, allowing users to trade off between search accuracy and computational efficiency.<br>  ⭐ Indexing structures: FAISS implements several indexing structures to accelerate nearest neighbor search. The most commonly used index structures in FAISS are the Inverted File (IVF) and Product Quantization (PQ) indexes, which are designed to handle high-dimensional vectors efficiently.<br>  ⭐ GPU support: FAISS leverages the computational power of GPUs to accelerate similarity search operations. By utilizing GPUs, FAISS can achieve significant speedups compared to CPU-based implementations, especially for large-scale datasets.<br>  ⭐ Integration with deep learning frameworks: FAISS provides integration with popular deep learning frameworks like PyTorch and TensorFlow. This allows seamless integration of FAISS-based similarity search into existing deep learning pipelines.<br>  ⭐ Extensibility: FAISS is designed to be modular and extensible, allowing users to customize and extend its functionality based on their specific requirements. It provides a flexible API that supports different indexing methods, distance metrics, and search parameters.<br>  ⭐ Community support: FAISS has a vibrant community of users and developers who actively contribute to its development and provide support. It is an open-source project hosted on GitHub, making it accessible to the wider community for contributions and improvements.<br> <h1>Steps for finding Similar Embedding Vector of An Image</h1> Let’s use a use case of finding similar embedding vectors for an image of Ari.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*j9_rRtdpaRax10H8VTFfZw.jpeg\" width=\"95%\"></center><span>Here are the general steps involved:<br>  ⭐ Preparing the dataset : First, you need to have a dataset of images and their corresponding embedding vectors. These embedding vectors are typically obtained by passing the images through a pre-trained deep learning model, such as a convolutional neural network (CNN), which extracts meaningful features from the images.<br>  ⭐ Creating an index : Once you have the dataset and the embedding vectors, you need to create an index using FAISS. The index is a data structure that organizes the vectors in a way that facilitates efficient similarity search. FAISS provides various index structures, but for this example, we’ll use the Inverted File (IVF) index.<br>  ⭐ Adding vectors to the index : You add the embedding vectors of your dataset to the index using the `add` function provided by FAISS. This step builds the index structure based on the given vectors, making subsequent search operations faster.<br>  ⭐ Performing a similarity search : Now that the index is ready, you can perform a similarity search to find the most similar vectors to the image of Ari. You start by obtaining the embedding vector for Ari’s image using the same pre-trained model you used for the dataset.<br>  ⭐ Querying the index : Using the embedding vector of Ari, you pass it to the index’s `search` function in FAISS. This function will efficiently find the nearest neighbors (similar vectors) to the query vector based on a specified distance metric.<br>  ⭐ Analyzing the results : FAISS returns the nearest neighbors along with their distances to the query vector. You can analyze these results to identify the most similar images to Ari based on their embedding vectors. The distances can serve as a measure of similarity, with smaller distances indicating closer similarity.<br>  By following these steps, you can leverage FAISS to efficiently search and find similar embedding vectors for the image of Ari. This process can be applied to a wide range of applications, such as content-based image retrieval, recommendation systems, and more.<br> <h1>An Example</h1>  Our task here is to find out the most similar embedding vector from an index for a given query vector. First, let’s import FAISS using the following command.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*8RQNpBaXx2VPdWMRE4xmZQ.png\" width=\"95%\"></center><span>Secondly, let’s get embedding vectors for each images and store’em all.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*lSzsbvtEU1rDUq4O42GWag.jpeg\" width=\"95%\"></center><span>After we get these embeddings, we have to build an index. I would build an index for the first embeddings collection.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*CyVaNbmvf7ydYxviON0BIw.png\" width=\"95%\"></center><span>After building an index, let’s check out! I would take the first embedding vector of second embeddings collection as the query vector. labels2 has the corresponding labels. As we take the 0th element, the label is labels2[0].  When we search, two array values are returned. First array (D) holds the distances and the second array (I) holds the positions of the embedding vectors which are found to be so close. The first value in each are considered to be very close to the query. Let’s check whether the labels1[I[0][0]] equals to labels2[0] or not.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*_h4LMBtJwln8fx8sLiMaHg.png\" width=\"95%\"></center><span>True they are…<br> <h1>Index Types</h1>  FAISS provides various types of indexes that can be used for different similarity search scenarios. Here are a few commonly used index types in FAISS :<br>  ⭐ IndexFlatL2 : This index type implements a brute-force approach for similarity search using the L2 (Euclidean) distance metric. It compares the query vector with all vectors in the index to find the nearest neighbors. IndexFlatL2 is simple and easy to use, but it can be computationally expensive for large datasets. <br> ⭐ IndexIVFFlat : The Inverted File (IVF) index is designed for approximate nearest neighbor search. It divides the vector space into multiple cells and assigns vectors to these cells. IndexIVFFlat uses an inverted file structure to store these assignments efficiently. During search, only a subset of cells is examined, which significantly speeds up the search process. <br> ⭐ IndexHNSWFlat : The Hierarchical Navigable Small World (HNSW) index is a graph-based index that organizes vectors in a hierarchical manner. It builds a graph where each vector is connected to other vectors based on their proximity. This index structure allows for efficient and scalable approximate nearest neighbor search. <br> ⭐ IndexLSH : The Locality-Sensitive Hashing (LSH) index is suitable for very high-dimensional vectors. It uses hashing techniques to map similar vectors to the same or nearby buckets. During search, only the relevant buckets are examined, reducing the search space. IndexLSH provides approximate nearest neighbor search with trade-offs between accuracy and speed. <br> These are just a few examples of index types available in Faiss. Depending on your specific use case, we can choose the appropriate index type and configure its parameters to achieve the desired balance between search accuracy and computational efficiency.<br> <h1>Implementation of All</h1></span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*M5R6pFOgM3c4I-2bbSq42w.png\" width=\"95%\"></center><span>Let’s check which one performs well for this usecase.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*9LOI0FJSXXOUi5jDQO-_Dg.png\" width=\"95%\"></center><span><h1>Under The Hood</h1>      1.Multi-threading to exploit multiple cores and perform parallel searches on multiple GPUs.     <br>2.BLAS libraries for efficient exact distance computations via matrix/matrix multiplication. An efficient brute-force implementation cannot be optimal without using BLAS. BLAS/LAPACK is the only mandatory software dependency of FAISS.<br>     3.Machine SIMD vectorization and popcount are used to speed up distance computations for isolated vectors. <br> <h1>MERCI</h1></span><br></span>"
    ]
  },
  {
    "title": "FAISS Library ",
    "logo": "https://miro.medium.com/v2/resize:fit:640/format:webp/1*5nN-752sNXdmOshueJZaCg.png",
    "topic": "FAISS",
    "num": "3",
    "feeds": [
      "27-06-2023 ",
      "<span><center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*5nN-752sNXdmOshueJZaCg.png\" width=\"95%\"></center><span>FAISS is an open-source library developed by Facebook AI Research (FAIR) that provides efficient similarity search and clustering algorithms for high-dimensional vectors. The name “FAISS” stands for “Facebook AI Similarity Search.” It is specifically designed to handle large-scale datasets and is widely used in various applications, including image and text retrieval, recommender systems, and computer vision tasks. <br><h3>Key Features</h3> ⭐ Fast similarity search: FAISS offers highly optimized algorithms for efficient similarity search. It supports both exact and approximate nearest neighbor search, allowing users to trade off between search accuracy and computational efficiency.<br>  ⭐ Indexing structures: FAISS implements several indexing structures to accelerate nearest neighbor search. The most commonly used index structures in FAISS are the Inverted File (IVF) and Product Quantization (PQ) indexes, which are designed to handle high-dimensional vectors efficiently.<br>  ⭐ GPU support: FAISS leverages the computational power of GPUs to accelerate similarity search operations. By utilizing GPUs, FAISS can achieve significant speedups compared to CPU-based implementations, especially for large-scale datasets.<br>  ⭐ Integration with deep learning frameworks: FAISS provides integration with popular deep learning frameworks like PyTorch and TensorFlow. This allows seamless integration of FAISS-based similarity search into existing deep learning pipelines.<br>  ⭐ Extensibility: FAISS is designed to be modular and extensible, allowing users to customize and extend its functionality based on their specific requirements. It provides a flexible API that supports different indexing methods, distance metrics, and search parameters.<br>  ⭐ Community support: FAISS has a vibrant community of users and developers who actively contribute to its development and provide support. It is an open-source project hosted on GitHub, making it accessible to the wider community for contributions and improvements.<br> <h1>Steps for finding Similar Embedding Vector of An Image</h1> Let’s use a use case of finding similar embedding vectors for an image of Ari.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*j9_rRtdpaRax10H8VTFfZw.jpeg\" width=\"95%\"></center><span>Here are the general steps involved:<br>  ⭐ Preparing the dataset : First, you need to have a dataset of images and their corresponding embedding vectors. These embedding vectors are typically obtained by passing the images through a pre-trained deep learning model, such as a convolutional neural network (CNN), which extracts meaningful features from the images.<br>  ⭐ Creating an index : Once you have the dataset and the embedding vectors, you need to create an index using FAISS. The index is a data structure that organizes the vectors in a way that facilitates efficient similarity search. FAISS provides various index structures, but for this example, we’ll use the Inverted File (IVF) index.<br>  ⭐ Adding vectors to the index : You add the embedding vectors of your dataset to the index using the `add` function provided by FAISS. This step builds the index structure based on the given vectors, making subsequent search operations faster.<br>  ⭐ Performing a similarity search : Now that the index is ready, you can perform a similarity search to find the most similar vectors to the image of Ari. You start by obtaining the embedding vector for Ari’s image using the same pre-trained model you used for the dataset.<br>  ⭐ Querying the index : Using the embedding vector of Ari, you pass it to the index’s `search` function in FAISS. This function will efficiently find the nearest neighbors (similar vectors) to the query vector based on a specified distance metric.<br>  ⭐ Analyzing the results : FAISS returns the nearest neighbors along with their distances to the query vector. You can analyze these results to identify the most similar images to Ari based on their embedding vectors. The distances can serve as a measure of similarity, with smaller distances indicating closer similarity.<br>  By following these steps, you can leverage FAISS to efficiently search and find similar embedding vectors for the image of Ari. This process can be applied to a wide range of applications, such as content-based image retrieval, recommendation systems, and more.<br> <h1>An Example</h1>  Our task here is to find out the most similar embedding vector from an index for a given query vector. First, let’s import FAISS using the following command.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*8RQNpBaXx2VPdWMRE4xmZQ.png\" width=\"95%\"></center><span>Secondly, let’s get embedding vectors for each images and store’em all.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*lSzsbvtEU1rDUq4O42GWag.jpeg\" width=\"95%\"></center><span>After we get these embeddings, we have to build an index. I would build an index for the first embeddings collection.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*CyVaNbmvf7ydYxviON0BIw.png\" width=\"95%\"></center><span>After building an index, let’s check out! I would take the first embedding vector of second embeddings collection as the query vector. labels2 has the corresponding labels. As we take the 0th element, the label is labels2[0].  When we search, two array values are returned. First array (D) holds the distances and the second array (I) holds the positions of the embedding vectors which are found to be so close. The first value in each are considered to be very close to the query. Let’s check whether the labels1[I[0][0]] equals to labels2[0] or not.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*_h4LMBtJwln8fx8sLiMaHg.png\" width=\"95%\"></center><span>True they are…<br> <h1>Index Types</h1>  FAISS provides various types of indexes that can be used for different similarity search scenarios. Here are a few commonly used index types in FAISS :<br>  ⭐ IndexFlatL2 : This index type implements a brute-force approach for similarity search using the L2 (Euclidean) distance metric. It compares the query vector with all vectors in the index to find the nearest neighbors. IndexFlatL2 is simple and easy to use, but it can be computationally expensive for large datasets. <br> ⭐ IndexIVFFlat : The Inverted File (IVF) index is designed for approximate nearest neighbor search. It divides the vector space into multiple cells and assigns vectors to these cells. IndexIVFFlat uses an inverted file structure to store these assignments efficiently. During search, only a subset of cells is examined, which significantly speeds up the search process. <br> ⭐ IndexHNSWFlat : The Hierarchical Navigable Small World (HNSW) index is a graph-based index that organizes vectors in a hierarchical manner. It builds a graph where each vector is connected to other vectors based on their proximity. This index structure allows for efficient and scalable approximate nearest neighbor search. <br> ⭐ IndexLSH : The Locality-Sensitive Hashing (LSH) index is suitable for very high-dimensional vectors. It uses hashing techniques to map similar vectors to the same or nearby buckets. During search, only the relevant buckets are examined, reducing the search space. IndexLSH provides approximate nearest neighbor search with trade-offs between accuracy and speed. <br> These are just a few examples of index types available in Faiss. Depending on your specific use case, we can choose the appropriate index type and configure its parameters to achieve the desired balance between search accuracy and computational efficiency.<br> <h1>Implementation of All</h1></span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*M5R6pFOgM3c4I-2bbSq42w.png\" width=\"95%\"></center><span>Let’s check which one performs well for this usecase.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*9LOI0FJSXXOUi5jDQO-_Dg.png\" width=\"95%\"></center><span><h1>Under The Hood</h1>      1.Multi-threading to exploit multiple cores and perform parallel searches on multiple GPUs.     <br>2.BLAS libraries for efficient exact distance computations via matrix/matrix multiplication. An efficient brute-force implementation cannot be optimal without using BLAS. BLAS/LAPACK is the only mandatory software dependency of FAISS.<br>     3.Machine SIMD vectorization and popcount are used to speed up distance computations for isolated vectors. <br> <h1>MERCI</h1></span><br></span>"
    ]
  },
  {
    "title": "FAISS Library ",
    "logo": "https://miro.medium.com/v2/resize:fit:640/format:webp/1*5nN-752sNXdmOshueJZaCg.png",
    "topic": "FAISS",
    "num": "4",
    "feeds": [
      "27-06-2023 ",
      "<span><center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*5nN-752sNXdmOshueJZaCg.png\" width=\"95%\"></center><span>FAISS is an open-source library developed by Facebook AI Research (FAIR) that provides efficient similarity search and clustering algorithms for high-dimensional vectors. The name “FAISS” stands for “Facebook AI Similarity Search.” It is specifically designed to handle large-scale datasets and is widely used in various applications, including image and text retrieval, recommender systems, and computer vision tasks. <br><h3>Key Features</h3> ⭐ Fast similarity search: FAISS offers highly optimized algorithms for efficient similarity search. It supports both exact and approximate nearest neighbor search, allowing users to trade off between search accuracy and computational efficiency.<br>  ⭐ Indexing structures: FAISS implements several indexing structures to accelerate nearest neighbor search. The most commonly used index structures in FAISS are the Inverted File (IVF) and Product Quantization (PQ) indexes, which are designed to handle high-dimensional vectors efficiently.<br>  ⭐ GPU support: FAISS leverages the computational power of GPUs to accelerate similarity search operations. By utilizing GPUs, FAISS can achieve significant speedups compared to CPU-based implementations, especially for large-scale datasets.<br>  ⭐ Integration with deep learning frameworks: FAISS provides integration with popular deep learning frameworks like PyTorch and TensorFlow. This allows seamless integration of FAISS-based similarity search into existing deep learning pipelines.<br>  ⭐ Extensibility: FAISS is designed to be modular and extensible, allowing users to customize and extend its functionality based on their specific requirements. It provides a flexible API that supports different indexing methods, distance metrics, and search parameters.<br>  ⭐ Community support: FAISS has a vibrant community of users and developers who actively contribute to its development and provide support. It is an open-source project hosted on GitHub, making it accessible to the wider community for contributions and improvements.<br> <h1>Steps for finding Similar Embedding Vector of An Image</h1> Let’s use a use case of finding similar embedding vectors for an image of Ari.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*j9_rRtdpaRax10H8VTFfZw.jpeg\" width=\"95%\"></center><span>Here are the general steps involved:<br>  ⭐ Preparing the dataset : First, you need to have a dataset of images and their corresponding embedding vectors. These embedding vectors are typically obtained by passing the images through a pre-trained deep learning model, such as a convolutional neural network (CNN), which extracts meaningful features from the images.<br>  ⭐ Creating an index : Once you have the dataset and the embedding vectors, you need to create an index using FAISS. The index is a data structure that organizes the vectors in a way that facilitates efficient similarity search. FAISS provides various index structures, but for this example, we’ll use the Inverted File (IVF) index.<br>  ⭐ Adding vectors to the index : You add the embedding vectors of your dataset to the index using the `add` function provided by FAISS. This step builds the index structure based on the given vectors, making subsequent search operations faster.<br>  ⭐ Performing a similarity search : Now that the index is ready, you can perform a similarity search to find the most similar vectors to the image of Ari. You start by obtaining the embedding vector for Ari’s image using the same pre-trained model you used for the dataset.<br>  ⭐ Querying the index : Using the embedding vector of Ari, you pass it to the index’s `search` function in FAISS. This function will efficiently find the nearest neighbors (similar vectors) to the query vector based on a specified distance metric.<br>  ⭐ Analyzing the results : FAISS returns the nearest neighbors along with their distances to the query vector. You can analyze these results to identify the most similar images to Ari based on their embedding vectors. The distances can serve as a measure of similarity, with smaller distances indicating closer similarity.<br>  By following these steps, you can leverage FAISS to efficiently search and find similar embedding vectors for the image of Ari. This process can be applied to a wide range of applications, such as content-based image retrieval, recommendation systems, and more.<br> <h1>An Example</h1>  Our task here is to find out the most similar embedding vector from an index for a given query vector. First, let’s import FAISS using the following command.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*8RQNpBaXx2VPdWMRE4xmZQ.png\" width=\"95%\"></center><span>Secondly, let’s get embedding vectors for each images and store’em all.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*lSzsbvtEU1rDUq4O42GWag.jpeg\" width=\"95%\"></center><span>After we get these embeddings, we have to build an index. I would build an index for the first embeddings collection.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*CyVaNbmvf7ydYxviON0BIw.png\" width=\"95%\"></center><span>After building an index, let’s check out! I would take the first embedding vector of second embeddings collection as the query vector. labels2 has the corresponding labels. As we take the 0th element, the label is labels2[0].  When we search, two array values are returned. First array (D) holds the distances and the second array (I) holds the positions of the embedding vectors which are found to be so close. The first value in each are considered to be very close to the query. Let’s check whether the labels1[I[0][0]] equals to labels2[0] or not.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*_h4LMBtJwln8fx8sLiMaHg.png\" width=\"95%\"></center><span>True they are…<br> <h1>Index Types</h1>  FAISS provides various types of indexes that can be used for different similarity search scenarios. Here are a few commonly used index types in FAISS :<br>  ⭐ IndexFlatL2 : This index type implements a brute-force approach for similarity search using the L2 (Euclidean) distance metric. It compares the query vector with all vectors in the index to find the nearest neighbors. IndexFlatL2 is simple and easy to use, but it can be computationally expensive for large datasets. <br> ⭐ IndexIVFFlat : The Inverted File (IVF) index is designed for approximate nearest neighbor search. It divides the vector space into multiple cells and assigns vectors to these cells. IndexIVFFlat uses an inverted file structure to store these assignments efficiently. During search, only a subset of cells is examined, which significantly speeds up the search process. <br> ⭐ IndexHNSWFlat : The Hierarchical Navigable Small World (HNSW) index is a graph-based index that organizes vectors in a hierarchical manner. It builds a graph where each vector is connected to other vectors based on their proximity. This index structure allows for efficient and scalable approximate nearest neighbor search. <br> ⭐ IndexLSH : The Locality-Sensitive Hashing (LSH) index is suitable for very high-dimensional vectors. It uses hashing techniques to map similar vectors to the same or nearby buckets. During search, only the relevant buckets are examined, reducing the search space. IndexLSH provides approximate nearest neighbor search with trade-offs between accuracy and speed. <br> These are just a few examples of index types available in Faiss. Depending on your specific use case, we can choose the appropriate index type and configure its parameters to achieve the desired balance between search accuracy and computational efficiency.<br> <h1>Implementation of All</h1></span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*M5R6pFOgM3c4I-2bbSq42w.png\" width=\"95%\"></center><span>Let’s check which one performs well for this usecase.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*9LOI0FJSXXOUi5jDQO-_Dg.png\" width=\"95%\"></center><span><h1>Under The Hood</h1>      1.Multi-threading to exploit multiple cores and perform parallel searches on multiple GPUs.     <br>2.BLAS libraries for efficient exact distance computations via matrix/matrix multiplication. An efficient brute-force implementation cannot be optimal without using BLAS. BLAS/LAPACK is the only mandatory software dependency of FAISS.<br>     3.Machine SIMD vectorization and popcount are used to speed up distance computations for isolated vectors. <br> <h1>MERCI</h1></span><br></span>"
    ]
  },
  {
    "title": "FAISS Library ",
    "logo": "https://miro.medium.com/v2/resize:fit:640/format:webp/1*5nN-752sNXdmOshueJZaCg.png",
    "topic": "FAISS",
    "num": "5",
    "feeds": [
      "27-06-2023 ",
      "<span><center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*5nN-752sNXdmOshueJZaCg.png\" width=\"95%\"></center><span>FAISS is an open-source library developed by Facebook AI Research (FAIR) that provides efficient similarity search and clustering algorithms for high-dimensional vectors. The name “FAISS” stands for “Facebook AI Similarity Search.” It is specifically designed to handle large-scale datasets and is widely used in various applications, including image and text retrieval, recommender systems, and computer vision tasks. <br><h3>Key Features</h3> ⭐ Fast similarity search: FAISS offers highly optimized algorithms for efficient similarity search. It supports both exact and approximate nearest neighbor search, allowing users to trade off between search accuracy and computational efficiency.<br>  ⭐ Indexing structures: FAISS implements several indexing structures to accelerate nearest neighbor search. The most commonly used index structures in FAISS are the Inverted File (IVF) and Product Quantization (PQ) indexes, which are designed to handle high-dimensional vectors efficiently.<br>  ⭐ GPU support: FAISS leverages the computational power of GPUs to accelerate similarity search operations. By utilizing GPUs, FAISS can achieve significant speedups compared to CPU-based implementations, especially for large-scale datasets.<br>  ⭐ Integration with deep learning frameworks: FAISS provides integration with popular deep learning frameworks like PyTorch and TensorFlow. This allows seamless integration of FAISS-based similarity search into existing deep learning pipelines.<br>  ⭐ Extensibility: FAISS is designed to be modular and extensible, allowing users to customize and extend its functionality based on their specific requirements. It provides a flexible API that supports different indexing methods, distance metrics, and search parameters.<br>  ⭐ Community support: FAISS has a vibrant community of users and developers who actively contribute to its development and provide support. It is an open-source project hosted on GitHub, making it accessible to the wider community for contributions and improvements.<br> <h1>Steps for finding Similar Embedding Vector of An Image</h1> Let’s use a use case of finding similar embedding vectors for an image of Ari.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*j9_rRtdpaRax10H8VTFfZw.jpeg\" width=\"95%\"></center><span>Here are the general steps involved:<br>  ⭐ Preparing the dataset : First, you need to have a dataset of images and their corresponding embedding vectors. These embedding vectors are typically obtained by passing the images through a pre-trained deep learning model, such as a convolutional neural network (CNN), which extracts meaningful features from the images.<br>  ⭐ Creating an index : Once you have the dataset and the embedding vectors, you need to create an index using FAISS. The index is a data structure that organizes the vectors in a way that facilitates efficient similarity search. FAISS provides various index structures, but for this example, we’ll use the Inverted File (IVF) index.<br>  ⭐ Adding vectors to the index : You add the embedding vectors of your dataset to the index using the `add` function provided by FAISS. This step builds the index structure based on the given vectors, making subsequent search operations faster.<br>  ⭐ Performing a similarity search : Now that the index is ready, you can perform a similarity search to find the most similar vectors to the image of Ari. You start by obtaining the embedding vector for Ari’s image using the same pre-trained model you used for the dataset.<br>  ⭐ Querying the index : Using the embedding vector of Ari, you pass it to the index’s `search` function in FAISS. This function will efficiently find the nearest neighbors (similar vectors) to the query vector based on a specified distance metric.<br>  ⭐ Analyzing the results : FAISS returns the nearest neighbors along with their distances to the query vector. You can analyze these results to identify the most similar images to Ari based on their embedding vectors. The distances can serve as a measure of similarity, with smaller distances indicating closer similarity.<br>  By following these steps, you can leverage FAISS to efficiently search and find similar embedding vectors for the image of Ari. This process can be applied to a wide range of applications, such as content-based image retrieval, recommendation systems, and more.<br> <h1>An Example</h1>  Our task here is to find out the most similar embedding vector from an index for a given query vector. First, let’s import FAISS using the following command.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*8RQNpBaXx2VPdWMRE4xmZQ.png\" width=\"95%\"></center><span>Secondly, let’s get embedding vectors for each images and store’em all.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*lSzsbvtEU1rDUq4O42GWag.jpeg\" width=\"95%\"></center><span>After we get these embeddings, we have to build an index. I would build an index for the first embeddings collection.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*CyVaNbmvf7ydYxviON0BIw.png\" width=\"95%\"></center><span>After building an index, let’s check out! I would take the first embedding vector of second embeddings collection as the query vector. labels2 has the corresponding labels. As we take the 0th element, the label is labels2[0].  When we search, two array values are returned. First array (D) holds the distances and the second array (I) holds the positions of the embedding vectors which are found to be so close. The first value in each are considered to be very close to the query. Let’s check whether the labels1[I[0][0]] equals to labels2[0] or not.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*_h4LMBtJwln8fx8sLiMaHg.png\" width=\"95%\"></center><span>True they are…<br> <h1>Index Types</h1>  FAISS provides various types of indexes that can be used for different similarity search scenarios. Here are a few commonly used index types in FAISS :<br>  ⭐ IndexFlatL2 : This index type implements a brute-force approach for similarity search using the L2 (Euclidean) distance metric. It compares the query vector with all vectors in the index to find the nearest neighbors. IndexFlatL2 is simple and easy to use, but it can be computationally expensive for large datasets. <br> ⭐ IndexIVFFlat : The Inverted File (IVF) index is designed for approximate nearest neighbor search. It divides the vector space into multiple cells and assigns vectors to these cells. IndexIVFFlat uses an inverted file structure to store these assignments efficiently. During search, only a subset of cells is examined, which significantly speeds up the search process. <br> ⭐ IndexHNSWFlat : The Hierarchical Navigable Small World (HNSW) index is a graph-based index that organizes vectors in a hierarchical manner. It builds a graph where each vector is connected to other vectors based on their proximity. This index structure allows for efficient and scalable approximate nearest neighbor search. <br> ⭐ IndexLSH : The Locality-Sensitive Hashing (LSH) index is suitable for very high-dimensional vectors. It uses hashing techniques to map similar vectors to the same or nearby buckets. During search, only the relevant buckets are examined, reducing the search space. IndexLSH provides approximate nearest neighbor search with trade-offs between accuracy and speed. <br> These are just a few examples of index types available in Faiss. Depending on your specific use case, we can choose the appropriate index type and configure its parameters to achieve the desired balance between search accuracy and computational efficiency.<br> <h1>Implementation of All</h1></span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*M5R6pFOgM3c4I-2bbSq42w.png\" width=\"95%\"></center><span>Let’s check which one performs well for this usecase.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*9LOI0FJSXXOUi5jDQO-_Dg.png\" width=\"95%\"></center><span><h1>Under The Hood</h1>      1.Multi-threading to exploit multiple cores and perform parallel searches on multiple GPUs.     <br>2.BLAS libraries for efficient exact distance computations via matrix/matrix multiplication. An efficient brute-force implementation cannot be optimal without using BLAS. BLAS/LAPACK is the only mandatory software dependency of FAISS.<br>     3.Machine SIMD vectorization and popcount are used to speed up distance computations for isolated vectors. <br> <h1>MERCI</h1></span><br></span>"
    ]
  }
]