[{
		"title": "Tamilar Ulagam ",
		"logo": "https://media4.giphy.com/media/73E4wQO3OUZPO/200w.gif?cid=82a1493bezs0tirfoorlx9tbhfeu3lfn3x7yrs0o3vrj3seq&rid=200w.gif&ct=g",
		"topic": "Jagged Array with Mr.Bean",
		"num": "0",
		"feeds": ["23-04-2023", "<span>For a single like me, watching Mr.Bean is an essential part of life. In one episode, Bean can’t sleep as usual. There is no Mosquito-Problem but Cat Problem. He gets rid of it by disguising himself as a Dog and barking to deceive the cats. He knows how to solve a problem. (Whether to apply a stack, or a heap , or socks)</span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*42n_Pz3cccW_sWIH5oGC6Q.jpeg\" width=\"95%\"></center>\n<span>Then, he tries to sleep but he can’t. So, he starts counting sheep in a photo. He gets messed with the count. So, to calculate the total number, he uses a calculator and computed row multiplied by the column considering the sheep as an n dimensional array. The problem here is, the column size varies from row to row.</span><br>\n<center><div class=\"iframe\"><iframe width=\"100%\" height=\"100%\" src=\"https://www.youtube.com/embed/FmbmNp1RDCE?controls=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe></div></center>\n<span>He doesn’t care of it. He simply computes the RxC and sleeps. ( Somehow, he slept and the END ) It remembered me the so-called Jagged Array / Ragged Array.So, What is it ? Jagged arrays are the arrays with variable column size. </span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*YXl3u6SjRrzzj0QcjNBMFw.png\" width=\"95%\"></center>\n<span>If we have an array of arrays, we usually work with a fixed size of column. But, there are some situations when we don’t have fixed column size. This is the so called Jagged Array structure. Let’s declare a ragged array in Java. </span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*R7HbbriaoF0DjxUbHOR-dA.png\" width=\"95%\"></center>\n<span>As you can see, we declare the array to have 3 rows and a non-fixed number of columns. Now, let’s say what are the column sizes in different rows. </span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*yER6Prw2DbHI8UzeitlbJA.png\" width=\"95%\"></center>\n<span>So, in first row, there will be 5 elements. And the second row would have 3 and the third would have 2.Now, let’s initialize them using for-loops.</span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*MXSgGARZU6qpErstshrHfQ.png\" width=\"95%\"></center>\n<span>Now, let’s pick the elements of the ragged guy dynamically. It can be used to store rows of data of varying lengths to improve performance when working with multi-dimensional arrays.</span><br>\n<center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*RBcREM1LDs_ZzTH6TG3Uig.png\" width=\"95%\"></center>\n<span>USES : It can be used to store rows of data of varying lengths to improve performance when working with multi-dimensional arrays. </span><br>\n"]
	},
	{
		"title": "Tamilar Ulagam ",
		"logo": "https://miro.medium.com/v2/resize:fit:640/format:webp/1*5nN-752sNXdmOshueJZaCg.png",
		"topic": "FAISS",
		"num": "1",
		"feeds": ["27-06-2023 ", "<span><center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*5nN-752sNXdmOshueJZaCg.png\" width=\"95%\"></center><span>FAISS is an open-source library developed by Facebook AI Research (FAIR) that provides efficient similarity search and clustering algorithms for high-dimensional vectors. The name “FAISS” stands for “Facebook AI Similarity Search.” It is specifically designed to handle large-scale datasets and is widely used in various applications, including image and text retrieval, recommender systems, and computer vision tasks. <br><h3>Key Features</h3> ⭐ Fast similarity search: FAISS offers highly optimized algorithms for efficient similarity search. It supports both exact and approximate nearest neighbor search, allowing users to trade off between search accuracy and computational efficiency.<br>  ⭐ Indexing structures: FAISS implements several indexing structures to accelerate nearest neighbor search. The most commonly used index structures in FAISS are the Inverted File (IVF) and Product Quantization (PQ) indexes, which are designed to handle high-dimensional vectors efficiently.<br>  ⭐ GPU support: FAISS leverages the computational power of GPUs to accelerate similarity search operations. By utilizing GPUs, FAISS can achieve significant speedups compared to CPU-based implementations, especially for large-scale datasets.<br>  ⭐ Integration with deep learning frameworks: FAISS provides integration with popular deep learning frameworks like PyTorch and TensorFlow. This allows seamless integration of FAISS-based similarity search into existing deep learning pipelines.<br>  ⭐ Extensibility: FAISS is designed to be modular and extensible, allowing users to customize and extend its functionality based on their specific requirements. It provides a flexible API that supports different indexing methods, distance metrics, and search parameters.<br>  ⭐ Community support: FAISS has a vibrant community of users and developers who actively contribute to its development and provide support. It is an open-source project hosted on GitHub, making it accessible to the wider community for contributions and improvements.<br> <h1>Steps for finding Similar Embedding Vector of An Image</h1> Let’s use a use case of finding similar embedding vectors for an image of Ari.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*j9_rRtdpaRax10H8VTFfZw.jpeg\" width=\"95%\"></center><span>Here are the general steps involved:<br>  ⭐ Preparing the dataset : First, you need to have a dataset of images and their corresponding embedding vectors. These embedding vectors are typically obtained by passing the images through a pre-trained deep learning model, such as a convolutional neural network (CNN), which extracts meaningful features from the images.<br>  ⭐ Creating an index : Once you have the dataset and the embedding vectors, you need to create an index using FAISS. The index is a data structure that organizes the vectors in a way that facilitates efficient similarity search. FAISS provides various index structures, but for this example, we’ll use the Inverted File (IVF) index.<br>  ⭐ Adding vectors to the index : You add the embedding vectors of your dataset to the index using the `add` function provided by FAISS. This step builds the index structure based on the given vectors, making subsequent search operations faster.<br>  ⭐ Performing a similarity search : Now that the index is ready, you can perform a similarity search to find the most similar vectors to the image of Ari. You start by obtaining the embedding vector for Ari’s image using the same pre-trained model you used for the dataset.<br>  ⭐ Querying the index : Using the embedding vector of Ari, you pass it to the index’s `search` function in FAISS. This function will efficiently find the nearest neighbors (similar vectors) to the query vector based on a specified distance metric.<br>  ⭐ Analyzing the results : FAISS returns the nearest neighbors along with their distances to the query vector. You can analyze these results to identify the most similar images to Ari based on their embedding vectors. The distances can serve as a measure of similarity, with smaller distances indicating closer similarity.<br>  By following these steps, you can leverage FAISS to efficiently search and find similar embedding vectors for the image of Ari. This process can be applied to a wide range of applications, such as content-based image retrieval, recommendation systems, and more.<br> <h1>An Example</h1>  Our task here is to find out the most similar embedding vector from an index for a given query vector. First, let’s import FAISS using the following command.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*8RQNpBaXx2VPdWMRE4xmZQ.png\" width=\"95%\"></center><span>Secondly, let’s get embedding vectors for each images and store’em all.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*lSzsbvtEU1rDUq4O42GWag.jpeg\" width=\"95%\"></center><span>After we get these embeddings, we have to build an index. I would build an index for the first embeddings collection.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*CyVaNbmvf7ydYxviON0BIw.png\" width=\"95%\"></center><span>After building an index, let’s check out! I would take the first embedding vector of second embeddings collection as the query vector. labels2 has the corresponding labels. As we take the 0th element, the label is labels2[0].  When we search, two array values are returned. First array (D) holds the distances and the second array (I) holds the positions of the embedding vectors which are found to be so close. The first value in each are considered to be very close to the query. Let’s check whether the labels1[I[0][0]] equals to labels2[0] or not.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*_h4LMBtJwln8fx8sLiMaHg.png\" width=\"95%\"></center><span>True they are…<br> <h1>Index Types</h1>  FAISS provides various types of indexes that can be used for different similarity search scenarios. Here are a few commonly used index types in FAISS :<br>  ⭐ IndexFlatL2 : This index type implements a brute-force approach for similarity search using the L2 (Euclidean) distance metric. It compares the query vector with all vectors in the index to find the nearest neighbors. IndexFlatL2 is simple and easy to use, but it can be computationally expensive for large datasets. <br> ⭐ IndexIVFFlat : The Inverted File (IVF) index is designed for approximate nearest neighbor search. It divides the vector space into multiple cells and assigns vectors to these cells. IndexIVFFlat uses an inverted file structure to store these assignments efficiently. During search, only a subset of cells is examined, which significantly speeds up the search process. <br> ⭐ IndexHNSWFlat : The Hierarchical Navigable Small World (HNSW) index is a graph-based index that organizes vectors in a hierarchical manner. It builds a graph where each vector is connected to other vectors based on their proximity. This index structure allows for efficient and scalable approximate nearest neighbor search. <br> ⭐ IndexLSH : The Locality-Sensitive Hashing (LSH) index is suitable for very high-dimensional vectors. It uses hashing techniques to map similar vectors to the same or nearby buckets. During search, only the relevant buckets are examined, reducing the search space. IndexLSH provides approximate nearest neighbor search with trade-offs between accuracy and speed. <br> These are just a few examples of index types available in Faiss. Depending on your specific use case, we can choose the appropriate index type and configure its parameters to achieve the desired balance between search accuracy and computational efficiency.<br> <h1>Implementation of All</h1></span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*M5R6pFOgM3c4I-2bbSq42w.png\" width=\"95%\"></center><span>Let’s check which one performs well for this usecase.</span><br><center><img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*9LOI0FJSXXOUi5jDQO-_Dg.png\" width=\"95%\"></center><span><h1>Under The Hood</h1>      1.Multi-threading to exploit multiple cores and perform parallel searches on multiple GPUs.     <br>2.BLAS libraries for efficient exact distance computations via matrix/matrix multiplication. An efficient brute-force implementation cannot be optimal without using BLAS. BLAS/LAPACK is the only mandatory software dependency of FAISS.<br>     3.Machine SIMD vectorization and popcount are used to speed up distance computations for isolated vectors. <br> <h1>MERCI</h1></span><br></span>"]
	},
	{
		"title": "Neural Network ",
		"logo": "https://media3.giphy.com/media/xT9IgN8YKRhByRBzMI/giphy.gif?cid=6c09b952fssombz2mzcueme48zefw99n75u6ifr1wewupjvl&ep=v1_internal_gif_by_id&rid=giphy.gif&ct=g",
		"topic": "Neural Network ",
		"num": "2",
		"feeds": ["28-07-2023", "<span>Neural Network is indeed one of the most remarkable discoveries in human history. It operates in a fascinating and almost magical manner. Just like a baby learns from its experiences, a neural network learns from data. But what does learning mean in the context of a neural network? Let's explore this concept in simple terms.</span>\n<span>In the Southern Boy's 3D Animation, he illustrates how a neural network works, making the learning process more accessible. The golden spheres represent neurons, which are the building blocks of a neural network.</span><br>\n<div class=\"iframe\"><iframe width=\"100%\" height=\"100%\" src=\"https://www.youtube.com/embed/IwTxj6X36w8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe></div><br>\n<span>Learning in a neural network occurs through a process called \"training.\" During training, the network is exposed to a large amount of input data along with the corresponding correct output or label. The network's task is to learn patterns and relationships within the data so that it can accurately map inputs to the correct outputs.</span><br>\n<h3>USE-CASE : XOR</h3><hr>\n<span>Brace yourself for an enchanting adventure into the realm of neural networks, where intelligence and intuition converge in the pursuit of understanding this fascinating phenomenon. We all know what is XOR Operation. It gives 1 when inputs are different. It gives 0 when inputs are same.</span><br>\n<h3>LAYERS</h3><hr>\n<span>A Neural Network usually has three layers of Input Layer, Hidden Layer and Output Layer. There can be more hidden layers. They can be increased to a large extent for a subtle performance but it can also lead to some potential risks.</span><br>\n<h3>WEIGHTS AND BIASES : RANDOMNESS</h3><hr>\n<span>Imagine we are trying to tune our FM radio to find a specific channel (\"FM Rainbow\"). However, we have no idea where the exact frequency of this channel is, so we start with the radio knobs set to random positions. In the world of neural networks, similar to our radio tuning, we begin with random weights and biases. These weights and biases are like the radio knobs, but instead of controlling the radio frequency, they control the behavior of the neural network. So, initially, these weights and biases are randomly initialized.</span><br>\n<h3>FORWARD PROPAGATION</h3><hr>\n<span>Now, we turn on the radio and start scanning through different frequencies. As we twist the knobs randomly, we might not immediately land on the FM Rainbow channel. But every time we try a new random setting, we pay attention to the sound we hear. Gradually, we start adjusting the knobs based on the sound, moving closer and closer to the FM Rainbow channel. In a neural network, we do something similar during the learning process. The network starts with random weights and biases, and it takes in some input data.</span><br>\n<span>Each input is multiplied by its corresponding weight. These weights represent the strengths of connections between the inputs and the neuron. The multiplication of inputs and weights determines how much influence each input has on the neuron's output. The results of the multiplications (inputs multiplied by weights) are then added together to form a single value. In essence, the inputs are individually adjusted by their corresponding weights and then combined to create a single value, which serves as the input for the activation function.\nThe activation function in a neural network is like a switch that decides whether a neuron should be \"activated\" or \"turned on\" based on the input it receives. Just like how a light switch can either turn on the light or keep it off, the activation function determines if a neuron should fire or remain silent. When the input to a neuron passes through the activation function, it transforms the input in a certain way. If the transformed value is above a certain threshold, the neuron becomes active and sends its output to the next layer of the neural network. On the other hand, if the transformed value is below the threshold, the neuron stays inactive, and its output is not propagated further. Here, in our illustration, we use sigmoid activation function. There is a plethora of activation functions. This non-linear behavior introduced by the activation function allows neural networks to learn and capture complex patterns in the data. It's like introducing flexibility in decision-making, as neurons can either contribute significantly to the final output or not at all, depending on the transformed value.</span><br>\n<h3>LOSS</h3><hr>\n<span>Just like we kept tweaking the radio knobs and listened to the sound, the neural network does something similar with its predictions. It compares its predictions with the actual correct answers (labels) for the input data and measures how far off it was. This difference is called the \"error\" or \"loss.\" For XOR Operation, when inputs are 0 and 1, the output must be 1. But, what we get here is not 1. So, we measure the distance between the actual value and the predicted value to take a appropriate action. Here, Mean Squared Error is used. There is also a plethora of loss functions.</span><br>\n<h3>THE MAGIC</h3><hr>\n<span>The magic happens when the neural network uses a clever algorithm called \"backpropagation\" to adjust those random weights and biases based on the error it calculated. By doing this over and over again with different data, the network learns from its mistakes and gradually gets better at making predictions, just like we got closer to the FM Rainbow channel by fine-tuning the knobs.</span> <br>\n<pre style=\"color: #16FF00;\">\n1. Input Layer Neurons (x1=1, x2=0)\n   - Neuron x1: The input value is 1.\n   - Neuron x2: The input value is 0.\n\n2. Hidden Layer Neuron (h):\n   - Input: \n     - Neuron x1 sends its output (1)\n       through weight w1 (0.3).\n     - Neuron x2 sends its output (0)\n       through weight w2 (0.5).\n     - Adding bias b1 (0.1).\n     - Calculate the weighted sum:\n       z1 = (1 * 0.3) + (0 * 0.5) + 0.1\n          = 0.4\n   - Activation:\n     - The sigmoid activation function:\n       h = 1 / (1 + exp(-z1))\n       ≈ 0.59874\n\n3. Output Layer Neuron (o):\n   - Input: \n     - Neuron h sends its output (0.59874)\n       through weight w3 (0.2).\n     - Neuron h sends its output (0.59874)\n       through weight w4 (0.4).\n     - Adding bias b2 (0.6).\n     - Calculate the weighted sum:\n       z2 = (0.59874*0.2)+(0.59874*0.4)+0.6\n        ≈ 0.959244\n   - Activation:\n     - The sigmoid activation function:\n       o = 1 / (1 + exp(-z2)) ≈ 0.7222\n\n4. Loss Calculation:\n   - Given that the target output for XOR with\n     inputs (1, 0) is 1, the actual output is\n     approximately 0.7222.\n   - Calculate the Mean Squared Error loss:\n     L = (1/2) * (o - y_true)^2\n       ≈ (1/2) * (0.7222 - 1)^2\n       ≈ 0.03858\n\n5. Backpropagation:\n   - Derivative of the loss with respect to\n     the output layer input (z2):\n     ∂L/∂z2 = (o - y_true) * o * (1 - o)\n     ≈ (0.7222 - 1) * 0.7222 * (1 - 0.7222)\n     ≈ -0.14719\n\n   - Derivative of the loss with respect to\n     the hidden layer output (h):\n     ∂L/∂h = ∂L/∂z2 * w3\n     ≈ -0.14719 * 0.2 ≈ -0.02944\n\n   - Derivative of the loss with respect to\n     the input layer weights and bias:\n     ∂L/∂w1 = ∂L/∂h * h * (1 - h) * x1\n     ≈ -0.02944*0.59874*(1 - 0.59874)*1\n     ≈ -0.00783\n     ∂L/∂w2 = ∂L/∂h * h * (1 - h) * x2\n     ≈ -0.02944 * 0.59874*(1 - 0.59874)*0\n     ≈ 0\n     ∂L/∂b1 = ∂L/∂h * h * (1 - h)\n     ≈ -0.02944*0.59874 * (1 - 0.59874)\n     ≈ -0.01108\n\n   - Derivative of the loss w.r.to the o/p\n     layer weights and bias:\n     ∂L/∂w3 = ∂L/∂z2 * h\n     ≈ -0.14719 * 0.59874\n     ≈ -0.08812\n     ∂L/∂w4 = ∂L/∂z2 * h\n     ≈-0.14719 * 0.59874\n     ≈ -0.08812\n     ∂L/∂b2 = ∂L/∂z2\n     ≈ -0.14719\n\n6. Update Weights and Biases:\n   - Learning rate α = 0.1.\n   - New weights and biases:\n     w1_new = w1 - α * ∂L/∂w1\n     ≈ 0.3 - 0.1 * (-0.00783)\n     ≈ 0.300783\n     w2_new = w2 - α * ∂L/∂w2\n     ≈ 0.5 - 0.1 * 0 ≈ 0.5\n     w3_new = w3 - α * ∂L/∂w3\n     ≈ 0.2 - 0.1 * (-0.08812) \n     ≈ 0.208812\n     w4_new = w4 - α * ∂L/∂w4 \n     ≈ 0.4 - 0.1 * (-0.08812) \n     ≈ 0.408812\n     b1_new = b1 - α * ∂L/∂b1 \n     ≈ 0.1 - 0.1 * (-0.01108) \n     ≈ 0.100111\n     b2_new = b2 - α * ∂L/∂b2 \n     ≈ 0.6 - 0.1 * (-0.14719) \n     ≈ 0.614719\n </pre>\n<br>\n<span>These updated values of weights and biases are used for forward propagation.</span><br>\n<span>Eventually, after many rounds of adjusting and fine-tuning, the neural network's weights and biases settle into values that help it make highly accurate predictions for the FM Rainbow channel (or any other task it was designed for). This process of finding the best weights and biases for the neural network is what we call \"training,\" and it allows the neural network to uncover hidden patterns and relationships in the data, much like how you discovered the FM Rainbow channel by tuning your radio.</span><br>"]
	},

	{
		"title": "Key Knows ",
		"logo": "https://img.freepik.com/premium-vector/key-logo-design-template-secure-symbol_18099-4057.jpg",
		"topic": "Key Knows ",
		"num": "3",
		"feeds": ["13-12-2023", "<span><span>KEY and Pretraining in TRANSFORMERS... 🗝</span><br><b>QUERY:</b> I am the vector representing the word of interest.<br><b>KEY:</b> I am the vector holding the information on how much attention should be paid to each word with respect to QUERY.<br><b>VALUE:</b> I am the vector of the word of interest that gets weighted after knowing the contextual information of QUERY.<br><br><span>Here, how does KEY know how much attention should be paid to each word? If we have a sentence like 'The tiger comes from the south, and it is the direction of divinity', what does the word 'it' in the sentence refer to? Apparently, it refers to the direction 'south' because the tiger is not a direction! It's an animal belonging to the Panther Family (panthera tigris). 🐯</span><br><span>When QUERY is 'Tiger' and KEY is the word 'it', the multiplication of QUERY and KEY will have a low value. However, when QUERY is 'South' and KEY is the word 'it', the multiplication of QUERY and KEY will have a high value. 🤠</span><br><span>But how? How does KEY instruct the learning correctly? How did KEY acquire this information, something akin to human-level understanding? The answer is during Pretraining! 😲</span><br><img src='https://raw.githubusercontent.com/arihara-sudhan/arihara-sudhan.github.io/main/statics/1701070443258.jpeg'><br><span>Pretraining is when the transformer learns how to determine the meaning of a word based on the CONTEXT! On a large chunk of data (corpus), pretraining is conducted to understand how words are aligned, how they are associated with each other, the underlying patterns, and so on. 😍</span><br><span><br>Yes, KEY information is learned during pretraining. 🤩</span><br><span>Stay connected and keep learning... ☘️</span></span>"]
	}

]
